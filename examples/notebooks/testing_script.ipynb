{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the Sequence Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mczeneszew/Desktop/FLIPv3/examples/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "      <th>set</th>\n",
       "      <th>validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-1.0, -1.0, -3.0, -2.0, 0.0, -3.0, -2.0, 1.0...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-1.0, -1.0, -3.0, -2.0, 0.0, -3.0, -2.0, 1.0...</td>\n",
       "      <td>1.445905</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-1.0, -1.0, -3.0, -2.0, 0.0, -3.0, -2.0, 1.0...</td>\n",
       "      <td>1.690164</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-1.0, -1.0, -3.0, -2.0, 0.0, -3.0, -2.0, 1.0...</td>\n",
       "      <td>1.170550</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-1.0, -1.0, -3.0, -2.0, 0.0, -3.0, -2.0, 1.0...</td>\n",
       "      <td>2.401243</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[-1.0, -1.0, -3.0, -2.0, 0.0, -3.0, -2.0, 1.0...</td>\n",
       "      <td>1.838187</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[-1.0, -1.0, -3.0, -2.0, 0.0, -3.0, -2.0, 1.0...</td>\n",
       "      <td>1.372949</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[-1.0, -1.0, -3.0, -2.0, 0.0, -3.0, -2.0, 1.0...</td>\n",
       "      <td>1.413936</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[-1.0, -1.0, -3.0, -2.0, 0.0, -3.0, -2.0, 1.0...</td>\n",
       "      <td>0.820788</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[-1.0, -1.0, -3.0, -2.0, 0.0, -3.0, -2.0, 1.0...</td>\n",
       "      <td>0.585649</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence    target    set  \\\n",
       "0  [[-1.0, -1.0, -3.0, -2.0, 0.0, -3.0, -2.0, 1.0...  1.000000   test   \n",
       "1  [[-1.0, -1.0, -3.0, -2.0, 0.0, -3.0, -2.0, 1.0...  1.445905   test   \n",
       "2  [[-1.0, -1.0, -3.0, -2.0, 0.0, -3.0, -2.0, 1.0...  1.690164   test   \n",
       "3  [[-1.0, -1.0, -3.0, -2.0, 0.0, -3.0, -2.0, 1.0...  1.170550   test   \n",
       "4  [[-1.0, -1.0, -3.0, -2.0, 0.0, -3.0, -2.0, 1.0...  2.401243   test   \n",
       "5  [[-1.0, -1.0, -3.0, -2.0, 0.0, -3.0, -2.0, 1.0...  1.838187   test   \n",
       "6  [[-1.0, -1.0, -3.0, -2.0, 0.0, -3.0, -2.0, 1.0...  1.372949   test   \n",
       "7  [[-1.0, -1.0, -3.0, -2.0, 0.0, -3.0, -2.0, 1.0...  1.413936   test   \n",
       "8  [[-1.0, -1.0, -3.0, -2.0, 0.0, -3.0, -2.0, 1.0...  0.820788  train   \n",
       "9  [[-1.0, -1.0, -3.0, -2.0, 0.0, -3.0, -2.0, 1.0...  0.585649  train   \n",
       "\n",
       "  validation  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        NaN  \n",
       "5        NaN  \n",
       "6        NaN  \n",
       "7        NaN  \n",
       "8        NaN  \n",
       "9       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "helpers_path = os.path.join(root_dir, 'helpers')\n",
    "if not os.path.exists(helpers_path):\n",
    "    raise FileNotFoundError(f\"Helpers directory not found at: {helpers_path}\")\n",
    "\n",
    "from flip.utils.sequence_database import SequenceDatabase, read_csv_to_sequencedatabase\n",
    "\n",
    "# CSV File Path \n",
    "csv_path = '../data/four_mutations_small.csv'\n",
    "\n",
    "# Loading the sequence database from the CSV file\n",
    "seq_db = read_csv_to_sequencedatabase(csv_path)\n",
    "\n",
    "# Displaying the loaded sequence database as a DataFrame\n",
    "display(seq_db.to_dataframe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of seq_db.sequences: <class 'list'>\n",
      "First sequence in seq_db.sequences: [[-1. -1. -3. ...  1. -1. -1.]\n",
      " [-1. -3.  0. ... -2. -2. -1.]\n",
      " [-2. -2. -3. ... -1.  2.  7.]\n",
      " ...\n",
      " [-2. -3. -1. ... -3. -2.  2.]\n",
      " [-2. -3. -1. ... -3. -2.  2.]\n",
      " [-2. -3. -1. ... -3. -2.  2.]]\n",
      "Type of first sequence: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of seq_db.sequences:\", type(seq_db.sequences))\n",
    "print(\"First sequence in seq_db.sequences:\", seq_db.sequences[0])\n",
    "print(\"Type of first sequence:\", type(seq_db.sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded sequences (BLOSUM62):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[-1., -1., -3., ...,  1., -1., -1.],\n",
       "        [-1., -3.,  0., ..., -2., -2., -1.],\n",
       "        [-2., -2., -3., ..., -1.,  2.,  7.],\n",
       "        ...,\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.],\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.],\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.]]),\n",
       " array([[-1., -1., -3., ...,  1., -1., -1.],\n",
       "        [-1., -3.,  0., ..., -2., -2., -1.],\n",
       "        [-2., -2., -3., ..., -1.,  2.,  7.],\n",
       "        ...,\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.],\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.],\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.]]),\n",
       " array([[-1., -1., -3., ...,  1., -1., -1.],\n",
       "        [-1., -3.,  0., ..., -2., -2., -1.],\n",
       "        [-2., -2., -3., ..., -1.,  2.,  7.],\n",
       "        ...,\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.],\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.],\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.]]),\n",
       " array([[-1., -1., -3., ...,  1., -1., -1.],\n",
       "        [-1., -3.,  0., ..., -2., -2., -1.],\n",
       "        [-2., -2., -3., ..., -1.,  2.,  7.],\n",
       "        ...,\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.],\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.],\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.]]),\n",
       " array([[-1., -1., -3., ...,  1., -1., -1.],\n",
       "        [-1., -3.,  0., ..., -2., -2., -1.],\n",
       "        [-2., -2., -3., ..., -1.,  2.,  7.],\n",
       "        ...,\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.],\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.],\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.]]),\n",
       " array([[-1., -1., -3., ...,  1., -1., -1.],\n",
       "        [-1., -3.,  0., ..., -2., -2., -1.],\n",
       "        [-2., -2., -3., ..., -1.,  2.,  7.],\n",
       "        ...,\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.],\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.],\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.]]),\n",
       " array([[-1., -1., -3., ...,  1., -1., -1.],\n",
       "        [-1., -3.,  0., ..., -2., -2., -1.],\n",
       "        [-2., -2., -3., ..., -1.,  2.,  7.],\n",
       "        ...,\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.],\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.],\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.]]),\n",
       " array([[-1., -1., -3., ...,  1., -1., -1.],\n",
       "        [-1., -3.,  0., ..., -2., -2., -1.],\n",
       "        [-2., -2., -3., ..., -1.,  2.,  7.],\n",
       "        ...,\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.],\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.],\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.]]),\n",
       " array([[-1., -1., -3., ...,  1., -1., -1.],\n",
       "        [-1., -3.,  0., ..., -2., -2., -1.],\n",
       "        [-2., -2., -3., ..., -1.,  2.,  7.],\n",
       "        ...,\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.],\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.],\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.]]),\n",
       " array([[-1., -1., -3., ...,  1., -1., -1.],\n",
       "        [-1., -3.,  0., ..., -2., -2., -1.],\n",
       "        [-2., -2., -3., ..., -1.,  2.,  7.],\n",
       "        ...,\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.],\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.],\n",
       "        [-2., -3., -1., ..., -3., -2.,  2.]])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Encoded sequences (BLOSUM62):\")\n",
    "display(seq_db.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.4459050863,\n",
       " 1.69016355375,\n",
       " 1.17054988281,\n",
       " 2.40124349574,\n",
       " 1.83818718188,\n",
       " 1.37294933339,\n",
       " 1.41393590647,\n",
       " 0.820788193905,\n",
       " 0.585648853727]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Targets:\")\n",
    "display(seq_db.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Dataloader  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of ProteinDataset: 10\n",
      "First item in ProteinDataset: (tensor([[-1., -1., -3.,  ...,  1., -1., -1.],\n",
      "        [-1., -3.,  0.,  ..., -2., -2., -1.],\n",
      "        [-2., -2., -3.,  ..., -1.,  2.,  7.],\n",
      "        ...,\n",
      "        [-2., -3., -1.,  ..., -3., -2.,  2.],\n",
      "        [-2., -3., -1.,  ..., -3., -2.,  2.],\n",
      "        [-2., -3., -1.,  ..., -3., -2.,  2.]]), tensor(1.))\n",
      "Created DataLoader:\n",
      "torch.Size([10, 265, 20]) tensor([265, 265, 265, 265, 265, 265, 265, 265, 265, 265]) tensor([2.4012, 0.5856, 1.4139, 1.8382, 1.0000, 1.4459, 1.1705, 1.6902, 1.3729,\n",
      "        0.8208])\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from sequence_database import SequenceDatabase, read_csv_to_sequencedatabase\n",
    "# from dataloader import create_dataloader, ProteinDataset\n",
    "# import numpy as np\n",
    "\n",
    "# protein_dataset = ProteinDataset(seq_db)\n",
    "# print(f\"Length of ProteinDataset: {len(protein_dataset)}\")\n",
    "# print(f\"First item in ProteinDataset: {protein_dataset[0]}\")\n",
    "\n",
    "# batch_size = 32\n",
    "\n",
    "# # Creating the DataLoader\n",
    "# dataloader = create_dataloader(protein_dataset, batch_size=batch_size)\n",
    "\n",
    "# print(\"Created DataLoader:\")\n",
    "# # for inputs, labels in dataloader:\n",
    "# #     display(inputs, labels)\n",
    "# for inputs, lengths, labels in dataloader:\n",
    "#     print(inputs.shape, lengths, labels)\n",
    "#     break  # Only printing the first batch \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the correct root directory to the Python path\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "# Verify if helpers directory is accessible\n",
    "helpers_path = os.path.join(root_dir, 'helpers')\n",
    "if not os.path.exists(helpers_path):\n",
    "    raise FileNotFoundError(f\"Helpers directory not found at: {helpers_path}\")\n",
    "\n",
    "# Import the necessary modules\n",
    "from flip.utils.sequence_database import SequenceDatabase, read_csv_to_sequencedatabase\n",
    "from flip.utils.dataloader import create_dataloader, ProteinDataset\n",
    "\n",
    "# Read the CSV and create SequenceDatabase\n",
    "csv_path = '../data/four_mutations_small.csv'\n",
    "seq_db = read_csv_to_sequencedatabase(csv_path)\n",
    "\n",
    "# Create ProteinDataset\n",
    "protein_dataset = ProteinDataset(seq_db)\n",
    "print(f\"Length of ProteinDataset: {len(protein_dataset)}\")\n",
    "print(f\"First item in ProteinDataset: {protein_dataset[0]}\")\n",
    "\n",
    "# Define batch size and create DataLoader\n",
    "batch_size = 32\n",
    "dataloader = create_dataloader(protein_dataset, batch_size=batch_size)\n",
    "\n",
    "print(\"Created DataLoader:\")\n",
    "# Print the first batch\n",
    "for inputs, lengths, labels in dataloader:\n",
    "    print(inputs.shape, lengths, labels)\n",
    "    break  # Only printing the first batch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train the CNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/mczeneszew/Desktop/FLIPv3/examples/notebooks\n",
      "Root directory to be added to Python path: /Users/mczeneszew/Desktop/FLIPv3\n",
      "Helpers directory exists: True\n",
      "Length of ProteinDataset: 10\n",
      "First item in ProteinDataset: (tensor([[-1., -1., -3.,  ...,  1., -1., -1.],\n",
      "        [-1., -3.,  0.,  ..., -2., -2., -1.],\n",
      "        [-2., -2., -3.,  ..., -1.,  2.,  7.],\n",
      "        ...,\n",
      "        [-2., -3., -1.,  ..., -3., -2.,  2.],\n",
      "        [-2., -3., -1.,  ..., -3., -2.,  2.],\n",
      "        [-2., -3., -1.,  ..., -3., -2.,  2.]]), tensor(1.))\n",
      "Created DataLoader:\n",
      "torch.Size([10, 265, 20]) tensor([265, 265, 265, 265, 265, 265, 265, 265, 265, 265]) tensor([1.8382, 1.6902, 0.5856, 1.0000, 0.8208, 1.3729, 1.1705, 1.4139, 2.4012,\n",
      "        1.4459])\n",
      "Input shape: torch.Size([10, 265, 20])\n",
      "Epoch 1, Loss: 2.8332607746124268\n",
      "Input shape: torch.Size([10, 265, 20])\n",
      "Epoch 2, Loss: 0.2508598864078522\n",
      "Input shape: torch.Size([10, 265, 20])\n",
      "Epoch 3, Loss: 0.3516028821468353\n",
      "Input shape: torch.Size([10, 265, 20])\n",
      "Epoch 4, Loss: 0.346372127532959\n",
      "Input shape: torch.Size([10, 265, 20])\n",
      "Epoch 5, Loss: 0.24216416478157043\n",
      "Input shape: torch.Size([10, 265, 20])\n",
      "Epoch 6, Loss: 0.33727169036865234\n",
      "Input shape: torch.Size([10, 265, 20])\n",
      "Epoch 7, Loss: 0.2488057166337967\n",
      "Input shape: torch.Size([10, 265, 20])\n",
      "Epoch 8, Loss: 0.29578307271003723\n",
      "Input shape: torch.Size([10, 265, 20])\n",
      "Epoch 9, Loss: 0.23452003300189972\n",
      "Input shape: torch.Size([10, 265, 20])\n",
      "Epoch 10, Loss: 0.28666922450065613\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_q/32dn15tx2w90m2l4r4vkjcpc0000gp/T/ipykernel_92308/2757212411.py:89: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs, dtype=torch.float32)\n",
      "/var/folders/_q/32dn15tx2w90m2l4r4vkjcpc0000gp/T/ipykernel_92308/2757212411.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  lengths = torch.tensor(lengths, dtype=torch.float32)\n",
      "/var/folders/_q/32dn15tx2w90m2l4r4vkjcpc0000gp/T/ipykernel_92308/2757212411.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "# Check the current working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Manually add the correct root directory to the Python path\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "print(\"Root directory to be added to Python path:\", root_dir)\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "# Verify if helpers directory is accessible\n",
    "helpers_path = os.path.join(root_dir, 'helpers')\n",
    "print(\"Helpers directory exists:\", os.path.exists(helpers_path))\n",
    "\n",
    "# Import the necessary modules\n",
    "from flip.utils.sequence_database import SequenceDatabase, read_csv_to_sequencedatabase\n",
    "from flip.utils.dataloader import create_dataloader, ProteinDataset\n",
    "from flip.models.models_flip import ConvNet\n",
    "\n",
    "# Read the CSV and create SequenceDatabase\n",
    "csv_path = '../data/four_mutations_small.csv'\n",
    "seq_db = read_csv_to_sequencedatabase(csv_path)\n",
    "\n",
    "# Create ProteinDataset\n",
    "protein_dataset = ProteinDataset(seq_db)\n",
    "print(f\"Length of ProteinDataset: {len(protein_dataset)}\")\n",
    "print(f\"First item in ProteinDataset: {protein_dataset[0]}\")\n",
    "\n",
    "# Define batch size and create DataLoader\n",
    "batch_size = 32\n",
    "dataloader = create_dataloader(protein_dataset, batch_size=batch_size)\n",
    "\n",
    "print(\"Created DataLoader:\")\n",
    "# Print the first batch\n",
    "for inputs, lengths, labels in dataloader:\n",
    "    print(inputs.shape, lengths, labels)\n",
    "    break  # Only printing the first batch\n",
    "\n",
    "# Model Training Script\n",
    "input_size = protein_dataset[0][0].shape[0]  \n",
    "sequence_length = protein_dataset[0][0].shape[1]  # Getting the sequence length from data \n",
    "\n",
    "cnn_model = ConvNet(input_size=input_size, sequence_length=sequence_length)\n",
    "\n",
    "# Defining loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Now, we can train the model\n",
    "cnn_model.train()\n",
    "for epoch in range(10):  # Setting number of epochs \n",
    "    for inputs, lengths, labels in dataloader:\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "        lengths = torch.tensor(lengths, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        print(\"Input shape:\", inputs.shape)  # Debug print\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn_model(inputs, lengths)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))  # Ensuring labels match output shape\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.5: Training the CNN model on sequences of different lengths   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "# Importing the necessary modules\n",
    "from flip.utils.sequence_database import SequenceDatabase, read_csv_to_sequencedatabase\n",
    "from flip.utils.dataloader import create_dataloader, ProteinDataset\n",
    "from flip.models.models_flip import ConvNet\n",
    "\n",
    "# Reading the CSV and creating SequenceDatabase\n",
    "csv_path = '../data/four_mutations_random_lengths.csv'\n",
    "seq_db = read_csv_to_sequencedatabase(csv_path)\n",
    "\n",
    "# Creating ProteinDataset\n",
    "protein_dataset = ProteinDataset(seq_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataLoader:\n",
      "torch.Size([10, 263, 20]) tensor([255, 258, 255, 255, 261, 259, 263, 260, 257, 256]) tensor([0.5856, 1.1705, 1.6902, 2.4012, 0.8208, 1.4139, 1.3729, 1.4459, 1.8382,\n",
      "        1.0000])\n",
      "Epoch 1/10, Loss: 2.194927930831909\n",
      "Epoch 2/10, Loss: 766.2628173828125\n",
      "Epoch 3/10, Loss: 21.116275787353516\n",
      "Epoch 4/10, Loss: 4.276553153991699\n",
      "Epoch 5/10, Loss: 3.6542675495147705\n",
      "Epoch 6/10, Loss: 3.2011959552764893\n",
      "Epoch 7/10, Loss: 2.6982626914978027\n",
      "Epoch 8/10, Loss: 2.1052613258361816\n",
      "Epoch 9/10, Loss: 1.2481050491333008\n",
      "Epoch 10/10, Loss: 0.5172885060310364\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "dataloader = create_dataloader(protein_dataset, batch_size=batch_size)\n",
    "\n",
    "print(\"Created DataLoader:\")\n",
    "for inputs, lengths, labels in dataloader:\n",
    "    print(inputs.shape, lengths, labels)\n",
    "    break  # Only printing the first batch\n",
    "\n",
    "# Model Training Script\n",
    "input_size = protein_dataset[0][0].shape[1]  # This is the number of features per amino acid\n",
    "sequence_length = max([len(seq) for seq in seq_db.sequences])  # Maximum sequence length in the dataset\n",
    "cnn_model = ConvNet(input_size=input_size, sequence_length=sequence_length)\n",
    "\n",
    "# Defining the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "cnn_model.train()\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):  \n",
    "    epoch_loss = 0.0 \n",
    "    \n",
    "    for inputs, lengths, labels in dataloader:\n",
    "        inputs = inputs.float().transpose(1, 2)\n",
    "        labels = labels.float().unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn_model(inputs, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    average_loss = epoch_loss / len(dataloader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {average_loss}')\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training the Ridge Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.10.2' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "from cuml.linear_model import Ridge\n",
    "\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "encoded_sequences_flattened = [seq.flatten() for seq in encoded_sequences]\n",
    "ridge_model.fit(encoded_sequences_flattened, targets)\n",
    "predictions = ridge_model.predict(encoded_sequences_flattened)\n",
    "mse = np.mean((predictions - targets) ** 2)\n",
    "print(f'Ridge Regression MSE: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.10.2' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "from cuml.linear_model import Ridge\n",
    "\n",
    "# Simplified dataset for testing\n",
    "encoded_sequences_flattened = np.array([seq.flatten()[:100] for seq in seq_db.sequences])\n",
    "\n",
    "# Converting numpy arrays to cupy arrays for GPU processing\n",
    "encoded_sequences_flattened = cp.asarray(encoded_sequences_flattened, dtype=cp.float32)\n",
    "targets = cp.asarray(seq_db.targets, dtype=cp.float32)\n",
    "\n",
    "# Checking the shapes and data types\n",
    "print(\"Encoded sequences shape:\", encoded_sequences_flattened.shape)\n",
    "print(\"Encoded sequences dtype:\", encoded_sequences_flattened.dtype)\n",
    "print(\"Targets shape:\", targets.shape)\n",
    "print(\"Targets dtype:\", targets.dtype)\n",
    "\n",
    "# Initializing the Ridge Regression model\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "\n",
    "# Fitting the model\n",
    "try:\n",
    "    ridge_model.fit(encoded_sequences_flattened, targets)\n",
    "except Exception as e:\n",
    "    print(\"Error during fitting:\", e)\n",
    "\n",
    "# Predicting the targets\n",
    "try:\n",
    "    predictions = ridge_model.predict(encoded_sequences_flattened)\n",
    "    predictions = cp.asnumpy(predictions)\n",
    "    targets = cp.asnumpy(targets)\n",
    "\n",
    "    # Calculating the Mean Squared Error (MSE)\n",
    "    mse = np.mean((predictions - targets) ** 2)\n",
    "    print(f'Ridge Regression MSE: {mse}')\n",
    "except Exception as e:\n",
    "    print(\"Error during prediction:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flip_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
